{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Questions - Internal - R6 AIML Labs .ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zUZjPnVXGz0Z","colab_type":"text"},"source":["# The Iris Dataset\n","The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.\n","\n","The dataset contains a set of 150 records under five attributes - petal length, petal width, sepal length, sepal width and species."]},{"cell_type":"markdown","metadata":{"id":"RMbmpriavLE9","colab_type":"text"},"source":["### Specifying the TensorFlow version\n","Running `import tensorflow` will import the default version (currently 1.x). You can use 2.x by running a cell with the `tensorflow_version` magic **before** you run `import tensorflow`."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fu8bUU__oa7h","outputId":"53eebcb2-28b2-4ea5-e13e-cbce81efff7c","executionInfo":{"status":"ok","timestamp":1580233880255,"user_tz":-330,"elapsed":1232,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%tensorflow_version 2.x"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bLz1Ckvfvn6D"},"source":["### Import TensorFlow\n","Once you have specified a version via this magic, you can run `import tensorflow` as normal and verify which version was imported as follows:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CWrzVTLOvn6M","outputId":"f4e9d0dd-5032-475d-f16a-1147fb97030d","executionInfo":{"status":"ok","timestamp":1580233885125,"user_tz":-330,"elapsed":3259,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import tensorflow as tf\n","print(tf.__version__)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["2.1.0-rc1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_uYeJgkNuXNC","colab_type":"text"},"source":["### Set random seed"]},{"cell_type":"code","metadata":{"id":"lcASNsewsfQX","colab_type":"code","colab":{}},"source":["tf.random.set_seed(42)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5-vVQBBqg7DI","colab_type":"text"},"source":["## Question 1"]},{"cell_type":"markdown","metadata":{"id":"kE0EDKvQhEIe","colab_type":"text"},"source":["### Import dataset\n","- Import iris dataset\n","- Import the dataset using sklearn library"]},{"cell_type":"code","metadata":{"id":"IOOWpD26Haq3","colab_type":"code","outputId":"ec4b2978-1493-4005-b6de-1e658c727766","executionInfo":{"status":"ok","timestamp":1580233891437,"user_tz":-330,"elapsed":1419,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9q45X_TWrUhU","colab_type":"code","outputId":"e8c8658f-b5dc-429e-ca54-bd6002d51e3c","executionInfo":{"status":"ok","timestamp":1580233894025,"user_tz":-330,"elapsed":1582,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import OneHotEncoder\n","from keras.utils import np_utils\n","import tensorflow as tf\n","from tensorflow import keras"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"d9g5daKvq2xm","colab_type":"code","colab":{}},"source":["#iris_data = pd.read_csv('/gdrive/My Drive/Colab Notebooks/R6/Internal/iris.csv')\n","#print(\"Shape: \",iris_data.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iaTKzLwWtBMI","colab_type":"code","colab":{}},"source":["# Import iris data from sklearn\n","from sklearn.datasets import load_iris\n","iris_df = load_iris()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G22ARuoWriy1","colab_type":"code","outputId":"221dc894-5bc1-4d9f-ef35-741d7650e9ad","executionInfo":{"status":"ok","timestamp":1580233904502,"user_tz":-330,"elapsed":742,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# note: it is a Bunch object\n","# this basically acts like a dictionary where you can treat the keys like attributes\n","iris_df.keys()"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"ta8YqInTh5v5","colab_type":"text"},"source":["## Question 2"]},{"cell_type":"markdown","metadata":{"id":"HERt3drbhX0i","colab_type":"text"},"source":["### Get features and label from the dataset in separate variable\n","- you can get the features using .data method\n","- you can get the features using .target method"]},{"cell_type":"code","metadata":{"id":"0cV-_qHAHyvE","colab_type":"code","colab":{}},"source":["X = iris_df.data\n","Y = iris_df.target"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qg1A2lkUjFak","colab_type":"text"},"source":["## Question 3"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3YErwYLCH0N_"},"source":["### Create train and test data\n","- use train_test_split to get train and test set\n","- set a random_state\n","- test_size: 0.25"]},{"cell_type":"code","metadata":{"id":"TYKNJL85h7pQ","colab_type":"code","colab":{}},"source":["X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n","N, D = X_train.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mPpKyZFQH6tW","colab_type":"code","outputId":"131ff008-da77-409d-afca-a4412caa2ecc","executionInfo":{"status":"ok","timestamp":1580233913793,"user_tz":-330,"elapsed":1287,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(N, D)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["112 4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"g0KVP17Ozaix","colab_type":"text"},"source":["## Question 4"]},{"cell_type":"markdown","metadata":{"id":"SIjqxbhWv1zv","colab_type":"text"},"source":["### One-hot encode the labels\n","- convert class vectors (integers) to binary class matrix\n","- convert labels\n","- number of classes: 3\n","- we are doing this to use categorical_crossentropy as loss"]},{"cell_type":"code","metadata":{"id":"REJIEnJYIBHf","colab_type":"code","outputId":"8d1b19ce-8c73-43f9-b5fc-b06e08c10a79","executionInfo":{"status":"ok","timestamp":1580233917781,"user_tz":-330,"elapsed":1196,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["iris_df.target_names"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"iG3lx_ypzO4A","colab_type":"code","colab":{}},"source":["# num_classes=3 because number of classess in target variable is 3.\n","y_train = np_utils.to_categorical(y_train,num_classes=3)\n","y_test = np_utils.to_categorical(y_test,num_classes=3)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ovjLyYzWkO9s"},"source":["## Question 5"]},{"cell_type":"markdown","metadata":{"id":"hbIFzoPNSyYo","colab_type":"text"},"source":["### Initialize a sequential model\n","- Define a sequential model"]},{"cell_type":"code","metadata":{"id":"qH8iHKlR0iOt","colab_type":"code","colab":{}},"source":["# Clear previous model\n","keras.backend.clear_session()\n","\n","#Initilization\n","model = keras.models.Sequential()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dGMy999vlacX"},"source":["## Question 6"]},{"cell_type":"markdown","metadata":{"id":"72ibK5Jxm8iL","colab_type":"text"},"source":["### Add a layer\n","- Use Dense Layer  with input shape of 4 (according to the feature set) and number of outputs set to 3\n","- Apply Softmax on Dense Layer outputs"]},{"cell_type":"code","metadata":{"id":"uZKrBNSRm_o9","colab_type":"code","colab":{}},"source":["model.add(keras.layers.Dense(4, input_shape=(D,), activation='relu')) #Input layer\n","model.add(keras.layers.Dense(4, input_shape=(D,), activation='relu')) #Hidden layer\n","model.add(keras.layers.Dense(3, input_shape=(D,), activation='softmax')) #Output layer"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i4uiTH8plmNX","colab_type":"text"},"source":["## Question 7"]},{"cell_type":"markdown","metadata":{"id":"yJL8n8vcSyYz","colab_type":"text"},"source":["### Compile the model\n","- Use SGD as Optimizer\n","- Use categorical_crossentropy as loss function\n","- Use accuracy as metrics"]},{"cell_type":"code","metadata":{"id":"Tc_-fjIEk1ve","colab_type":"code","colab":{}},"source":["sgdOptimizer = keras.optimizers.SGD(lr=0.001)\n","model.compile(optimizer=sgdOptimizer,loss='categorical_crossentropy',metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sihIGbRll_jT"},"source":["## Question 8"]},{"cell_type":"markdown","metadata":{"id":"54ZZCfNGlu0i","colab_type":"text"},"source":["### Summarize the model\n","- Check model layers\n","- Understand number of trainable parameters"]},{"cell_type":"code","metadata":{"id":"elER3F_4ln8n","colab_type":"code","outputId":"1f323357-cd56-47f8-ac06-732ba3c4f4e7","executionInfo":{"status":"ok","timestamp":1580233996634,"user_tz":-330,"elapsed":1214,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}},"colab":{"base_uri":"https://localhost:8080/","height":252}},"source":["model.summary()"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense (Dense)                (None, 4)                 20        \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 4)                 20        \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 3)                 15        \n","=================================================================\n","Total params: 55\n","Trainable params: 55\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2PiP7j3Vmj4p"},"source":["## Question 9"]},{"cell_type":"markdown","metadata":{"id":"rWdbfFCXmCHt","colab_type":"text"},"source":["### Fit the model\n","- Give train data as training features and labels\n","- Epochs: 100\n","- Give validation data as testing features and labels"]},{"cell_type":"code","metadata":{"id":"cO1c-5tjmBVZ","colab_type":"code","outputId":"2a01bfcb-79ae-430a-e890-0979c48a4d64","executionInfo":{"status":"ok","timestamp":1580234026154,"user_tz":-330,"elapsed":3319,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["r = model.fit(x=X_train, y=y_train, epochs=100, validation_data=(X_test, y_test))"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Train on 112 samples, validate on 38 samples\n","Epoch 1/100\n","112/112 [==============================] - 0s 211us/sample - loss: 0.7402 - accuracy: 0.7321 - val_loss: 0.7212 - val_accuracy: 0.7105\n","Epoch 2/100\n","112/112 [==============================] - 0s 182us/sample - loss: 0.7365 - accuracy: 0.7321 - val_loss: 0.7165 - val_accuracy: 0.7632\n","Epoch 3/100\n","112/112 [==============================] - 0s 260us/sample - loss: 0.7328 - accuracy: 0.7411 - val_loss: 0.7123 - val_accuracy: 0.7632\n","Epoch 4/100\n","112/112 [==============================] - 0s 183us/sample - loss: 0.7293 - accuracy: 0.7411 - val_loss: 0.7084 - val_accuracy: 0.7632\n","Epoch 5/100\n","112/112 [==============================] - 0s 165us/sample - loss: 0.7261 - accuracy: 0.7411 - val_loss: 0.7045 - val_accuracy: 0.7632\n","Epoch 6/100\n","112/112 [==============================] - 0s 203us/sample - loss: 0.7226 - accuracy: 0.7679 - val_loss: 0.7007 - val_accuracy: 0.7632\n","Epoch 7/100\n","112/112 [==============================] - 0s 187us/sample - loss: 0.7195 - accuracy: 0.7679 - val_loss: 0.6966 - val_accuracy: 0.7632\n","Epoch 8/100\n","112/112 [==============================] - 0s 173us/sample - loss: 0.7159 - accuracy: 0.7679 - val_loss: 0.6930 - val_accuracy: 0.7632\n","Epoch 9/100\n","112/112 [==============================] - 0s 164us/sample - loss: 0.7132 - accuracy: 0.7679 - val_loss: 0.6895 - val_accuracy: 0.7632\n","Epoch 10/100\n","112/112 [==============================] - 0s 176us/sample - loss: 0.7097 - accuracy: 0.7768 - val_loss: 0.6859 - val_accuracy: 0.7632\n","Epoch 11/100\n","112/112 [==============================] - 0s 166us/sample - loss: 0.7063 - accuracy: 0.7768 - val_loss: 0.6822 - val_accuracy: 0.7632\n","Epoch 12/100\n","112/112 [==============================] - 0s 163us/sample - loss: 0.7034 - accuracy: 0.7768 - val_loss: 0.6789 - val_accuracy: 0.7632\n","Epoch 13/100\n","112/112 [==============================] - 0s 182us/sample - loss: 0.7001 - accuracy: 0.7768 - val_loss: 0.6754 - val_accuracy: 0.7632\n","Epoch 14/100\n","112/112 [==============================] - 0s 178us/sample - loss: 0.6969 - accuracy: 0.7857 - val_loss: 0.6716 - val_accuracy: 0.7632\n","Epoch 15/100\n","112/112 [==============================] - 0s 180us/sample - loss: 0.6936 - accuracy: 0.7768 - val_loss: 0.6680 - val_accuracy: 0.7632\n","Epoch 16/100\n","112/112 [==============================] - 0s 172us/sample - loss: 0.6904 - accuracy: 0.7857 - val_loss: 0.6646 - val_accuracy: 0.7895\n","Epoch 17/100\n","112/112 [==============================] - 0s 187us/sample - loss: 0.6876 - accuracy: 0.8036 - val_loss: 0.6610 - val_accuracy: 0.7632\n","Epoch 18/100\n","112/112 [==============================] - 0s 175us/sample - loss: 0.6848 - accuracy: 0.7857 - val_loss: 0.6577 - val_accuracy: 0.7895\n","Epoch 19/100\n","112/112 [==============================] - 0s 175us/sample - loss: 0.6817 - accuracy: 0.7946 - val_loss: 0.6544 - val_accuracy: 0.7632\n","Epoch 20/100\n","112/112 [==============================] - 0s 213us/sample - loss: 0.6794 - accuracy: 0.7857 - val_loss: 0.6514 - val_accuracy: 0.7632\n","Epoch 21/100\n","112/112 [==============================] - 0s 199us/sample - loss: 0.6762 - accuracy: 0.8036 - val_loss: 0.6482 - val_accuracy: 0.7632\n","Epoch 22/100\n","112/112 [==============================] - 0s 205us/sample - loss: 0.6732 - accuracy: 0.7857 - val_loss: 0.6452 - val_accuracy: 0.8158\n","Epoch 23/100\n","112/112 [==============================] - 0s 212us/sample - loss: 0.6703 - accuracy: 0.7946 - val_loss: 0.6421 - val_accuracy: 0.8158\n","Epoch 24/100\n","112/112 [==============================] - 0s 186us/sample - loss: 0.6676 - accuracy: 0.8036 - val_loss: 0.6390 - val_accuracy: 0.8158\n","Epoch 25/100\n","112/112 [==============================] - 0s 199us/sample - loss: 0.6649 - accuracy: 0.8036 - val_loss: 0.6362 - val_accuracy: 0.7895\n","Epoch 26/100\n","112/112 [==============================] - 0s 192us/sample - loss: 0.6624 - accuracy: 0.8036 - val_loss: 0.6331 - val_accuracy: 0.8158\n","Epoch 27/100\n","112/112 [==============================] - 0s 156us/sample - loss: 0.6595 - accuracy: 0.8036 - val_loss: 0.6301 - val_accuracy: 0.8158\n","Epoch 28/100\n","112/112 [==============================] - 0s 177us/sample - loss: 0.6569 - accuracy: 0.8036 - val_loss: 0.6273 - val_accuracy: 0.8158\n","Epoch 29/100\n","112/112 [==============================] - 0s 169us/sample - loss: 0.6544 - accuracy: 0.8036 - val_loss: 0.6244 - val_accuracy: 0.8158\n","Epoch 30/100\n","112/112 [==============================] - 0s 209us/sample - loss: 0.6518 - accuracy: 0.8036 - val_loss: 0.6216 - val_accuracy: 0.8421\n","Epoch 31/100\n","112/112 [==============================] - 0s 190us/sample - loss: 0.6494 - accuracy: 0.8036 - val_loss: 0.6187 - val_accuracy: 0.8421\n","Epoch 32/100\n","112/112 [==============================] - 0s 192us/sample - loss: 0.6469 - accuracy: 0.8036 - val_loss: 0.6160 - val_accuracy: 0.8421\n","Epoch 33/100\n","112/112 [==============================] - 0s 195us/sample - loss: 0.6446 - accuracy: 0.8125 - val_loss: 0.6134 - val_accuracy: 0.8421\n","Epoch 34/100\n","112/112 [==============================] - 0s 216us/sample - loss: 0.6421 - accuracy: 0.8036 - val_loss: 0.6110 - val_accuracy: 0.8421\n","Epoch 35/100\n","112/112 [==============================] - 0s 202us/sample - loss: 0.6399 - accuracy: 0.8125 - val_loss: 0.6085 - val_accuracy: 0.8421\n","Epoch 36/100\n","112/112 [==============================] - 0s 199us/sample - loss: 0.6373 - accuracy: 0.8125 - val_loss: 0.6058 - val_accuracy: 0.8421\n","Epoch 37/100\n","112/112 [==============================] - 0s 173us/sample - loss: 0.6350 - accuracy: 0.8036 - val_loss: 0.6031 - val_accuracy: 0.8421\n","Epoch 38/100\n","112/112 [==============================] - 0s 188us/sample - loss: 0.6331 - accuracy: 0.8125 - val_loss: 0.6004 - val_accuracy: 0.8421\n","Epoch 39/100\n","112/112 [==============================] - 0s 173us/sample - loss: 0.6308 - accuracy: 0.8125 - val_loss: 0.5980 - val_accuracy: 0.8421\n","Epoch 40/100\n","112/112 [==============================] - 0s 205us/sample - loss: 0.6281 - accuracy: 0.8125 - val_loss: 0.5957 - val_accuracy: 0.8421\n","Epoch 41/100\n","112/112 [==============================] - 0s 184us/sample - loss: 0.6262 - accuracy: 0.8125 - val_loss: 0.5933 - val_accuracy: 0.8421\n","Epoch 42/100\n","112/112 [==============================] - 0s 227us/sample - loss: 0.6239 - accuracy: 0.8125 - val_loss: 0.5908 - val_accuracy: 0.8421\n","Epoch 43/100\n","112/112 [==============================] - 0s 154us/sample - loss: 0.6215 - accuracy: 0.8125 - val_loss: 0.5883 - val_accuracy: 0.8421\n","Epoch 44/100\n","112/112 [==============================] - 0s 187us/sample - loss: 0.6195 - accuracy: 0.8214 - val_loss: 0.5860 - val_accuracy: 0.8421\n","Epoch 45/100\n","112/112 [==============================] - 0s 175us/sample - loss: 0.6180 - accuracy: 0.8125 - val_loss: 0.5837 - val_accuracy: 0.8421\n","Epoch 46/100\n","112/112 [==============================] - 0s 214us/sample - loss: 0.6153 - accuracy: 0.8214 - val_loss: 0.5814 - val_accuracy: 0.8421\n","Epoch 47/100\n","112/112 [==============================] - 0s 178us/sample - loss: 0.6131 - accuracy: 0.8214 - val_loss: 0.5792 - val_accuracy: 0.8421\n","Epoch 48/100\n","112/112 [==============================] - 0s 159us/sample - loss: 0.6107 - accuracy: 0.8214 - val_loss: 0.5771 - val_accuracy: 0.8421\n","Epoch 49/100\n","112/112 [==============================] - 0s 182us/sample - loss: 0.6094 - accuracy: 0.8214 - val_loss: 0.5749 - val_accuracy: 0.8421\n","Epoch 50/100\n","112/112 [==============================] - 0s 192us/sample - loss: 0.6070 - accuracy: 0.8214 - val_loss: 0.5725 - val_accuracy: 0.8158\n","Epoch 51/100\n","112/112 [==============================] - 0s 202us/sample - loss: 0.6050 - accuracy: 0.8214 - val_loss: 0.5705 - val_accuracy: 0.8158\n","Epoch 52/100\n","112/112 [==============================] - 0s 173us/sample - loss: 0.6028 - accuracy: 0.8304 - val_loss: 0.5684 - val_accuracy: 0.8421\n","Epoch 53/100\n","112/112 [==============================] - 0s 219us/sample - loss: 0.6008 - accuracy: 0.8214 - val_loss: 0.5664 - val_accuracy: 0.8421\n","Epoch 54/100\n","112/112 [==============================] - 0s 203us/sample - loss: 0.5991 - accuracy: 0.8214 - val_loss: 0.5643 - val_accuracy: 0.8158\n","Epoch 55/100\n","112/112 [==============================] - 0s 287us/sample - loss: 0.5973 - accuracy: 0.8304 - val_loss: 0.5622 - val_accuracy: 0.8158\n","Epoch 56/100\n","112/112 [==============================] - 0s 218us/sample - loss: 0.5953 - accuracy: 0.8393 - val_loss: 0.5601 - val_accuracy: 0.8158\n","Epoch 57/100\n","112/112 [==============================] - 0s 198us/sample - loss: 0.5933 - accuracy: 0.8304 - val_loss: 0.5582 - val_accuracy: 0.8421\n","Epoch 58/100\n","112/112 [==============================] - 0s 175us/sample - loss: 0.5915 - accuracy: 0.8214 - val_loss: 0.5562 - val_accuracy: 0.8158\n","Epoch 59/100\n","112/112 [==============================] - 0s 186us/sample - loss: 0.5896 - accuracy: 0.8393 - val_loss: 0.5543 - val_accuracy: 0.8158\n","Epoch 60/100\n","112/112 [==============================] - 0s 202us/sample - loss: 0.5880 - accuracy: 0.8482 - val_loss: 0.5525 - val_accuracy: 0.8421\n","Epoch 61/100\n","112/112 [==============================] - 0s 165us/sample - loss: 0.5862 - accuracy: 0.8304 - val_loss: 0.5505 - val_accuracy: 0.8421\n","Epoch 62/100\n","112/112 [==============================] - 0s 171us/sample - loss: 0.5843 - accuracy: 0.8304 - val_loss: 0.5485 - val_accuracy: 0.8421\n","Epoch 63/100\n","112/112 [==============================] - 0s 240us/sample - loss: 0.5827 - accuracy: 0.8393 - val_loss: 0.5469 - val_accuracy: 0.8421\n","Epoch 64/100\n","112/112 [==============================] - 0s 197us/sample - loss: 0.5814 - accuracy: 0.8214 - val_loss: 0.5449 - val_accuracy: 0.8158\n","Epoch 65/100\n","112/112 [==============================] - 0s 207us/sample - loss: 0.5792 - accuracy: 0.8393 - val_loss: 0.5431 - val_accuracy: 0.8158\n","Epoch 66/100\n","112/112 [==============================] - 0s 179us/sample - loss: 0.5775 - accuracy: 0.8482 - val_loss: 0.5412 - val_accuracy: 0.8158\n","Epoch 67/100\n","112/112 [==============================] - 0s 190us/sample - loss: 0.5757 - accuracy: 0.8661 - val_loss: 0.5396 - val_accuracy: 0.8158\n","Epoch 68/100\n","112/112 [==============================] - 0s 192us/sample - loss: 0.5740 - accuracy: 0.8571 - val_loss: 0.5377 - val_accuracy: 0.8158\n","Epoch 69/100\n","112/112 [==============================] - 0s 175us/sample - loss: 0.5724 - accuracy: 0.8393 - val_loss: 0.5358 - val_accuracy: 0.8158\n","Epoch 70/100\n","112/112 [==============================] - 0s 169us/sample - loss: 0.5707 - accuracy: 0.8661 - val_loss: 0.5341 - val_accuracy: 0.8158\n","Epoch 71/100\n","112/112 [==============================] - 0s 181us/sample - loss: 0.5691 - accuracy: 0.8661 - val_loss: 0.5324 - val_accuracy: 0.8158\n","Epoch 72/100\n","112/112 [==============================] - 0s 159us/sample - loss: 0.5677 - accuracy: 0.8661 - val_loss: 0.5307 - val_accuracy: 0.8158\n","Epoch 73/100\n","112/112 [==============================] - 0s 172us/sample - loss: 0.5659 - accuracy: 0.8661 - val_loss: 0.5289 - val_accuracy: 0.8158\n","Epoch 74/100\n","112/112 [==============================] - 0s 182us/sample - loss: 0.5645 - accuracy: 0.8661 - val_loss: 0.5272 - val_accuracy: 0.8158\n","Epoch 75/100\n","112/112 [==============================] - 0s 167us/sample - loss: 0.5626 - accuracy: 0.8661 - val_loss: 0.5256 - val_accuracy: 0.8158\n","Epoch 76/100\n","112/112 [==============================] - 0s 199us/sample - loss: 0.5611 - accuracy: 0.8661 - val_loss: 0.5238 - val_accuracy: 0.8158\n","Epoch 77/100\n","112/112 [==============================] - 0s 193us/sample - loss: 0.5595 - accuracy: 0.8661 - val_loss: 0.5222 - val_accuracy: 0.8421\n","Epoch 78/100\n","112/112 [==============================] - 0s 187us/sample - loss: 0.5580 - accuracy: 0.8661 - val_loss: 0.5205 - val_accuracy: 0.8421\n","Epoch 79/100\n","112/112 [==============================] - 0s 178us/sample - loss: 0.5565 - accuracy: 0.8661 - val_loss: 0.5191 - val_accuracy: 0.8158\n","Epoch 80/100\n","112/112 [==============================] - 0s 170us/sample - loss: 0.5552 - accuracy: 0.8661 - val_loss: 0.5177 - val_accuracy: 0.8158\n","Epoch 81/100\n","112/112 [==============================] - 0s 180us/sample - loss: 0.5535 - accuracy: 0.8661 - val_loss: 0.5161 - val_accuracy: 0.8158\n","Epoch 82/100\n","112/112 [==============================] - 0s 195us/sample - loss: 0.5522 - accuracy: 0.8661 - val_loss: 0.5144 - val_accuracy: 0.8421\n","Epoch 83/100\n","112/112 [==============================] - 0s 246us/sample - loss: 0.5506 - accuracy: 0.8661 - val_loss: 0.5129 - val_accuracy: 0.8421\n","Epoch 84/100\n","112/112 [==============================] - 0s 199us/sample - loss: 0.5495 - accuracy: 0.8661 - val_loss: 0.5118 - val_accuracy: 0.8158\n","Epoch 85/100\n","112/112 [==============================] - 0s 185us/sample - loss: 0.5480 - accuracy: 0.8661 - val_loss: 0.5099 - val_accuracy: 0.8421\n","Epoch 86/100\n","112/112 [==============================] - 0s 172us/sample - loss: 0.5464 - accuracy: 0.8839 - val_loss: 0.5086 - val_accuracy: 0.8421\n","Epoch 87/100\n","112/112 [==============================] - 0s 253us/sample - loss: 0.5449 - accuracy: 0.8661 - val_loss: 0.5072 - val_accuracy: 0.8421\n","Epoch 88/100\n","112/112 [==============================] - 0s 188us/sample - loss: 0.5436 - accuracy: 0.8661 - val_loss: 0.5057 - val_accuracy: 0.8421\n","Epoch 89/100\n","112/112 [==============================] - 0s 204us/sample - loss: 0.5422 - accuracy: 0.8750 - val_loss: 0.5042 - val_accuracy: 0.8421\n","Epoch 90/100\n","112/112 [==============================] - 0s 222us/sample - loss: 0.5408 - accuracy: 0.8929 - val_loss: 0.5029 - val_accuracy: 0.8421\n","Epoch 91/100\n","112/112 [==============================] - 0s 192us/sample - loss: 0.5397 - accuracy: 0.8661 - val_loss: 0.5015 - val_accuracy: 0.8421\n","Epoch 92/100\n","112/112 [==============================] - 0s 229us/sample - loss: 0.5383 - accuracy: 0.8750 - val_loss: 0.4999 - val_accuracy: 0.8421\n","Epoch 93/100\n","112/112 [==============================] - 0s 187us/sample - loss: 0.5374 - accuracy: 0.8839 - val_loss: 0.4984 - val_accuracy: 0.8421\n","Epoch 94/100\n","112/112 [==============================] - 0s 175us/sample - loss: 0.5356 - accuracy: 0.8929 - val_loss: 0.4971 - val_accuracy: 0.8421\n","Epoch 95/100\n","112/112 [==============================] - 0s 183us/sample - loss: 0.5346 - accuracy: 0.9018 - val_loss: 0.4956 - val_accuracy: 0.8421\n","Epoch 96/100\n","112/112 [==============================] - 0s 222us/sample - loss: 0.5332 - accuracy: 0.9018 - val_loss: 0.4943 - val_accuracy: 0.8421\n","Epoch 97/100\n","112/112 [==============================] - 0s 186us/sample - loss: 0.5316 - accuracy: 0.9018 - val_loss: 0.4929 - val_accuracy: 0.8421\n","Epoch 98/100\n","112/112 [==============================] - 0s 205us/sample - loss: 0.5307 - accuracy: 0.8929 - val_loss: 0.4916 - val_accuracy: 0.8421\n","Epoch 99/100\n","112/112 [==============================] - 0s 173us/sample - loss: 0.5294 - accuracy: 0.8929 - val_loss: 0.4903 - val_accuracy: 0.8421\n","Epoch 100/100\n","112/112 [==============================] - 0s 178us/sample - loss: 0.5280 - accuracy: 0.9018 - val_loss: 0.4891 - val_accuracy: 0.8421\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"re9ItAR3yS3J","colab_type":"text"},"source":["## Question 10"]},{"cell_type":"markdown","metadata":{"id":"liw0IFf9yVqH","colab_type":"text"},"source":["### Make predictions\n","- Predict labels on one row"]},{"cell_type":"code","metadata":{"id":"YCn70JqlJm7R","colab_type":"code","outputId":"47336cf0-0ee5-4716-90f4-33cd20ff1999","executionInfo":{"status":"ok","timestamp":1580234032124,"user_tz":-330,"elapsed":1241,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}},"colab":{"base_uri":"https://localhost:8080/","height":284}},"source":["import matplotlib.pyplot as plt\n","plt.plot(r.history['loss'], label='loss')\n","plt.plot(r.history['val_loss'], label='val_loss')\n","plt.legend()"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7f26fe78d7b8>"]},"metadata":{"tags":[]},"execution_count":31},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXoAAAD6CAYAAACvZ4z8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsHOS6IUQws5JohdCCDv3/0H9Y0W2+VxkAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"iV50r_t_KLop","colab_type":"code","outputId":"abcb82b6-d880-4aa1-dae7-bc302f93deeb","executionInfo":{"status":"ok","timestamp":1580234038426,"user_tz":-330,"elapsed":1696,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}},"colab":{"base_uri":"https://localhost:8080/","height":282}},"source":["# Plot the accuracy too\n","plt.plot(r.history['accuracy'], label='acc')\n","plt.plot(r.history['val_accuracy'], label='val_acc')\n","plt.legend()"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7f26fe792b38>"]},"metadata":{"tags":[]},"execution_count":32},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"H5sBybi6mlLl","colab_type":"code","outputId":"9f4f16bd-1da4-4f88-fea6-a7270f847212","executionInfo":{"status":"ok","timestamp":1580234042306,"user_tz":-330,"elapsed":1197,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Predictied Label\n","pred = model.predict(X_test)\n","pred = pd.DataFrame(pred)\n","predicted_label = pred.idxmax(axis=1)\n","print(\"Predicted label for each row: \", list(predicted_label))"],"execution_count":33,"outputs":[{"output_type":"stream","text":["Predicted label for each row:  [2, 0, 2, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 2, 1, 2, 2, 0, 2, 0, 2, 2, 1, 2, 2, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hSUgMq3m0bG7","colab_type":"text"},"source":["### Compare the prediction with actual label\n","- Print the same row as done in the previous step but of actual labels"]},{"cell_type":"code","metadata":{"id":"K5WbwVPyz-qQ","colab_type":"code","colab":{}},"source":["# Actual Label\n","y_test = pd.DataFrame(y_test)\n","actual_label = y_test.idxmax(axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IL70Jou_cKDj","colab_type":"code","outputId":"a19033b9-2fcb-49f2-cb25-e9f74d06b08a","executionInfo":{"status":"ok","timestamp":1580234048860,"user_tz":-330,"elapsed":1353,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}},"colab":{"base_uri":"https://localhost:8080/","height":195}},"source":["Combined_df = {\"Actual\": actual_label, \"Predicted\":predicted_label}\n","actual_and_predicted = pd.DataFrame(Combined_df)\n","actual_and_predicted.head()"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Actual</th>\n","      <th>Predicted</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Actual  Predicted\n","0       1          2\n","1       0          0\n","2       2          2\n","3       1          1\n","4       1          1"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"FrTKwbgE7NFT","colab_type":"text"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"a1UBYPNp5Tn1","colab_type":"text"},"source":["# Stock prices dataset\n","The data is of tock exchange's stock listings for each trading day of 2010 to 2016.\n","\n","## Description\n","A brief description of columns.\n","- open: The opening market price of the equity symbol on the date\n","- high: The highest market price of the equity symbol on the date\n","- low: The lowest recorded market price of the equity symbol on the date\n","- close: The closing recorded price of the equity symbol on the date\n","- symbol: Symbol of the listed company\n","- volume: Total traded volume of the equity symbol on the date\n","- date: Date of record"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ctH_ZW5g-M3g"},"source":["### Specifying the TensorFlow version\n","Running `import tensorflow` will import the default version (currently 1.x). You can use 2.x by running a cell with the `tensorflow_version` magic **before** you run `import tensorflow`."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vQbdODpH-M3r","colab":{}},"source":["%tensorflow_version 2.x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nFQWH1tj-M38"},"source":["### Import TensorFlow\n","Once you have specified a version via this magic, you can run `import tensorflow` as normal and verify which version was imported as follows:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Ho5n-xhd-M3_","outputId":"1b49b313-27d4-49b0-add8-c30ffd9a5d4e","executionInfo":{"status":"ok","timestamp":1580234055943,"user_tz":-330,"elapsed":1305,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import tensorflow as tf\n","print(tf.__version__)"],"execution_count":37,"outputs":[{"output_type":"stream","text":["2.1.0-rc1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tgkl0qu6-M4F"},"source":["### Set random seed"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TKgTyuA3-M4G","colab":{}},"source":["tf.random.set_seed(42)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_88voqAH-O6J","colab_type":"text"},"source":["## Question 1"]},{"cell_type":"markdown","metadata":{"id":"dRHCeJqP-evf","colab_type":"text"},"source":["### Load the data\n","- load the csv file and read it using pandas\n","- file name is prices.csv"]},{"cell_type":"code","metadata":{"id":"cKVH5v7r-RmC","colab_type":"code","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":74},"outputId":"42bd9331-fb76-4957-8ab5-05d4a6888ab4","executionInfo":{"status":"ok","timestamp":1580235234370,"user_tz":-330,"elapsed":934549,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}}},"source":["# run this cell to upload file if you are using google colab\n","from google.colab import files\n","files.upload()"],"execution_count":64,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-0e4e2701-07eb-4a1a-bb43-1d214e2a4c3f\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-0e4e2701-07eb-4a1a-bb43-1d214e2a4c3f\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving prices.csv to prices.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GFY23UaoXGQ9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"f7d46b63-0ba9-42d0-c769-949e3e9e589a","executionInfo":{"status":"ok","timestamp":1580235264149,"user_tz":-330,"elapsed":4821,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}}},"source":["!ls"],"execution_count":65,"outputs":[{"output_type":"stream","text":["prices.csv  sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"h8W9Na7ULBlq","colab_type":"code","colab":{}},"source":["keras.backend.clear_session()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-gDC6cSW_FSK","colab_type":"code","outputId":"2cf42d5c-ceac-4328-fbb2-e4a6e0d51019","executionInfo":{"status":"ok","timestamp":1580234072410,"user_tz":-330,"elapsed":2123,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["prices_df = pd.read_csv('/gdrive/My Drive/Colab Notebooks/R6/Internal/prices.csv')\n","print(\"Shape: \",prices_df.shape)"],"execution_count":41,"outputs":[{"output_type":"stream","text":["Shape:  (851264, 7)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HlLKVPVH_BCT","colab_type":"text"},"source":["## Question 2"]},{"cell_type":"markdown","metadata":{"id":"9J4BlzVA_gZd","colab_type":"text"},"source":["### Drop columnns\n","- drop \"date\" and \"symbol\" column from the data"]},{"cell_type":"code","metadata":{"id":"IKEK8aEE_Csx","colab_type":"code","outputId":"3155103a-ad96-4b20-c7ce-7cada44e8aef","executionInfo":{"status":"ok","timestamp":1580234075180,"user_tz":-330,"elapsed":1170,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}},"colab":{"base_uri":"https://localhost:8080/","height":195}},"source":["prices_df.head()"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>symbol</th>\n","      <th>open</th>\n","      <th>close</th>\n","      <th>low</th>\n","      <th>high</th>\n","      <th>volume</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2016-01-05 00:00:00</td>\n","      <td>WLTW</td>\n","      <td>123.430000</td>\n","      <td>125.839996</td>\n","      <td>122.309998</td>\n","      <td>126.250000</td>\n","      <td>2163600.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2016-01-06 00:00:00</td>\n","      <td>WLTW</td>\n","      <td>125.239998</td>\n","      <td>119.980003</td>\n","      <td>119.940002</td>\n","      <td>125.540001</td>\n","      <td>2386400.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2016-01-07 00:00:00</td>\n","      <td>WLTW</td>\n","      <td>116.379997</td>\n","      <td>114.949997</td>\n","      <td>114.930000</td>\n","      <td>119.739998</td>\n","      <td>2489500.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2016-01-08 00:00:00</td>\n","      <td>WLTW</td>\n","      <td>115.480003</td>\n","      <td>116.620003</td>\n","      <td>113.500000</td>\n","      <td>117.440002</td>\n","      <td>2006300.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2016-01-11 00:00:00</td>\n","      <td>WLTW</td>\n","      <td>117.010002</td>\n","      <td>114.970001</td>\n","      <td>114.089996</td>\n","      <td>117.330002</td>\n","      <td>1408600.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                  date symbol        open  ...         low        high     volume\n","0  2016-01-05 00:00:00   WLTW  123.430000  ...  122.309998  126.250000  2163600.0\n","1  2016-01-06 00:00:00   WLTW  125.239998  ...  119.940002  125.540001  2386400.0\n","2  2016-01-07 00:00:00   WLTW  116.379997  ...  114.930000  119.739998  2489500.0\n","3  2016-01-08 00:00:00   WLTW  115.480003  ...  113.500000  117.440002  2006300.0\n","4  2016-01-11 00:00:00   WLTW  117.010002  ...  114.089996  117.330002  1408600.0\n","\n","[5 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"code","metadata":{"id":"6PoPMV6QS_D2","colab_type":"code","outputId":"9d13b18c-f796-4ba7-91fe-245bc3b0c72c","executionInfo":{"status":"ok","timestamp":1580234078555,"user_tz":-330,"elapsed":1166,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}},"colab":{"base_uri":"https://localhost:8080/","height":195}},"source":["prices_df.drop(labels=\"date\", axis=1, inplace=True)\n","prices_df.drop(labels=\"symbol\", axis=1, inplace=True)\n","prices_df.head()"],"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>open</th>\n","      <th>close</th>\n","      <th>low</th>\n","      <th>high</th>\n","      <th>volume</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>123.430000</td>\n","      <td>125.839996</td>\n","      <td>122.309998</td>\n","      <td>126.250000</td>\n","      <td>2163600.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>125.239998</td>\n","      <td>119.980003</td>\n","      <td>119.940002</td>\n","      <td>125.540001</td>\n","      <td>2386400.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>116.379997</td>\n","      <td>114.949997</td>\n","      <td>114.930000</td>\n","      <td>119.739998</td>\n","      <td>2489500.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>115.480003</td>\n","      <td>116.620003</td>\n","      <td>113.500000</td>\n","      <td>117.440002</td>\n","      <td>2006300.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>117.010002</td>\n","      <td>114.970001</td>\n","      <td>114.089996</td>\n","      <td>117.330002</td>\n","      <td>1408600.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         open       close         low        high     volume\n","0  123.430000  125.839996  122.309998  126.250000  2163600.0\n","1  125.239998  119.980003  119.940002  125.540001  2386400.0\n","2  116.379997  114.949997  114.930000  119.739998  2489500.0\n","3  115.480003  116.620003  113.500000  117.440002  2006300.0\n","4  117.010002  114.970001  114.089996  117.330002  1408600.0"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"markdown","metadata":{"id":"cTPhO6v-AiZt","colab_type":"text"},"source":["## Question 3"]},{"cell_type":"markdown","metadata":{"id":"SsZXmF3NAkna","colab_type":"text"},"source":["### Take initial rows\n","- Take first 1000 rows from the data\n","- This step is done to make the execution faster"]},{"cell_type":"code","metadata":{"id":"aKs04iIHAjxN","colab_type":"code","outputId":"115450c4-fb77-487e-a969-d3aeb9895925","executionInfo":{"status":"ok","timestamp":1580234082550,"user_tz":-330,"elapsed":1428,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["prices_df = prices_df.head(1000)\n","prices_df.shape"],"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1000, 5)"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"markdown","metadata":{"id":"6vGtnapgBIJm","colab_type":"text"},"source":["## Question 4"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"C8u_jlbABTip"},"source":["### Get features and label from the dataset in separate variable\n","- Take \"open\", \"close\", \"low\", \"high\" columns as features\n","- Take \"volume\" column as label\n","- Normalize label column by dividing it with 1000000"]},{"cell_type":"code","metadata":{"id":"xQjCMzUXBJbg","colab_type":"code","colab":{}},"source":["X = prices_df.drop(labels=\"volume\",axis=1)\n","Y = prices_df[\"volume\"]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aTAKzlxZBz0z","colab_type":"text"},"source":["## Question 5"]},{"cell_type":"markdown","metadata":{"id":"IfY8Km1Zzyt2","colab_type":"text"},"source":["### Convert data\n","- Convert features and labels to numpy array\n","- Convert their data type to \"float32\""]},{"cell_type":"code","metadata":{"id":"Ko7nnQVbYENh","colab_type":"code","colab":{}},"source":["X = np.array(X)\n","Y = np.array(Y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m6SkAdUwepLb","colab_type":"code","outputId":"12078381-d1c5-4b3d-8321-0d7846106620","executionInfo":{"status":"ok","timestamp":1580234092274,"user_tz":-330,"elapsed":1191,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["print(\"X[0]: \",X[0])\n","print(\"Y[0]: \",Y[0])\n","print(\"Len of X:{} and length of Y:{}\".format(len(X), len(Y)))"],"execution_count":47,"outputs":[{"output_type":"stream","text":["X[0]:  [123.43     125.839996 122.309998 126.25    ]\n","Y[0]:  2163600.0\n","Len of X:1000 and length of Y:1000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3TWpN0nVTpUx"},"source":["## Question 6"]},{"cell_type":"markdown","metadata":{"id":"WQ1FKEs-4btX","colab_type":"text"},"source":["### Normalize data\n","- Normalize features\n","- Use tf.math.l2_normalize to normalize features\n","- You can read more about it here https://www.tensorflow.org/api_docs/python/tf/math/l2_normalize"]},{"cell_type":"code","metadata":{"id":"V0Tfe00X78wB","colab_type":"code","outputId":"ef5501e7-7aa9-418b-d9fa-3edf76b74ab4","executionInfo":{"status":"ok","timestamp":1580234096884,"user_tz":-330,"elapsed":1320,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}},"colab":{"base_uri":"https://localhost:8080/","height":151}},"source":["X_normalized = tf.math.l2_normalize(X,axis=0)\n","X_normalized"],"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1000, 4), dtype=float64, numpy=\n","array([[0.04405065, 0.0448811 , 0.04406771, 0.04469663],\n","       [0.04469661, 0.04279112, 0.04321381, 0.04444527],\n","       [0.04153459, 0.04099716, 0.04140873, 0.04239187],\n","       ...,\n","       [0.01010706, 0.01026088, 0.01009187, 0.01019968],\n","       [0.01570306, 0.01597802, 0.01576292, 0.01586421],\n","       [0.01287651, 0.01324606, 0.01297423, 0.01318064]])>"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wmXUGc2oTspa"},"source":["## Question 7"]},{"cell_type":"markdown","metadata":{"id":"VJelDMpzxs0L","colab_type":"text"},"source":["### Define weight and bias\n","- Initialize weight and bias with tf.zeros\n","- tf.zeros is an initializer that generates tensors initialized to 0\n","- Specify the value for shape"]},{"cell_type":"code","metadata":{"id":"U3sZzVZwPEXg","colab_type":"code","outputId":"9f193452-ac2d-47f2-ab9b-0c4323c7949b","executionInfo":{"status":"ok","timestamp":1580234101621,"user_tz":-330,"elapsed":1269,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["N,D = X_normalized.shape\n","D"],"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"code","metadata":{"id":"8o9RPWVTxs0O","colab_type":"code","colab":{}},"source":["#w = tf.Variable(tf.zeros(shape=(4,1)))\n","w = tf.Variable(tf.zeros(shape=(D,1)))\n","b = tf.Variable(tf.zeros(shape=(1)))\n","\n","# Change dtype of w and b to float64, if not changed then error will be observed during matrix multiplication and error calculation beacause X_normalized and Y is of type float64\n","w = tf.dtypes.cast(w, tf.float64)\n","b = tf.dtypes.cast(b, tf.float64)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vwr6-YgsL-x6","colab_type":"code","outputId":"b88cf1c7-9ea5-43da-a45c-d5114ebf23b9","executionInfo":{"status":"ok","timestamp":1580234108458,"user_tz":-330,"elapsed":1349,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}},"colab":{"base_uri":"https://localhost:8080/","height":118}},"source":["print(\"weight: \",w)\n","print(\"bias: \",b)"],"execution_count":51,"outputs":[{"output_type":"stream","text":["weight:  tf.Tensor(\n","[[0.]\n"," [0.]\n"," [0.]\n"," [0.]], shape=(4, 1), dtype=float64)\n","bias:  tf.Tensor([0.], shape=(1,), dtype=float64)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8a0wr94aTyjg"},"source":["## Question 8"]},{"cell_type":"markdown","metadata":{"id":"zMXXYdOSxs0Q","colab_type":"text"},"source":["### Get prediction\n","- Define a function to get prediction\n","- Approach: prediction = (X * W) + b; here is X is features"]},{"cell_type":"code","metadata":{"id":"U8Cty1y0xs0S","colab_type":"code","colab":{}},"source":["@tf.function\n","def predictionFunction(X, W, b):\n","  yhat = tf.add(tf.matmul(X, W), b)\n","  return yhat"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lQmS3Tauxs0V","colab_type":"text"},"source":["### Calculate loss\n","- Calculate loss using predictions\n","- Define a function to calculate loss\n","- We are calculating mean squared error"]},{"cell_type":"code","metadata":{"id":"-FRXmDd5xs0X","colab_type":"code","colab":{}},"source":["@tf.function\n","def loss_function(Y_actual, Y_predicted):\n","  error = Y_actual - Y_predicted\n","  loss = tf.reduce_mean(tf.square(error))\n","  return loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZbBpnOtfT0wd"},"source":["## Question 9"]},{"cell_type":"markdown","metadata":{"id":"bkOzAUUsTmF_","colab_type":"text"},"source":["### Define a function to train the model\n","1.   Record all the mathematical steps to calculate Loss\n","2.   Calculate Gradients of Loss w.r.t weights and bias\n","3.   Update Weights and Bias based on gradients and learning rate to minimize loss"]},{"cell_type":"code","metadata":{"id":"2R4uieGYLYtM","colab_type":"code","colab":{}},"source":["def train(X,Y, w,b,learning_rate = 0.1):\n","\n","  with tf.GradientTape() as tape:\n","    tape.watch([w,b])\n","    Yhat = predictionFunction(X, w,b)\n","    loss = loss_function(Y, Yhat)\n","  \n","  #evalute the gradient with the respect to the paramters\n","  dW, db = tape.gradient(loss, [w, b])\n","  \n","  #update the paramters using Gradient Descent  \n","  w = w - learning_rate * dW\n","  b = b - learning_rate * db\n","\n","  return w, b"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AW4SEP8kT2ls"},"source":["## Question 10"]},{"cell_type":"markdown","metadata":{"id":"yeN0deOvT81N","colab_type":"text"},"source":["### Train the model for 100 epochs \n","- Observe the training loss at every iteration"]},{"cell_type":"code","metadata":{"id":"Jjkn4gUgLevE","colab_type":"code","outputId":"83655d4e-638d-4057-8996-bfcb6dc6a86f","executionInfo":{"status":"ok","timestamp":1580234270759,"user_tz":-330,"elapsed":21236,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["epochs = 1001\n","for i in range(epochs):\n","  w, b = train(X_normalized,Y, w,b)\n","  print(\"Training Loss on iteration \", i, loss_function(Y, predictionFunction(X_normalized, w,b)).numpy())"],"execution_count":61,"outputs":[{"output_type":"stream","text":["Training Loss on iteration  0 209045761481272.8\n","Training Loss on iteration  1 209045761481238.12\n","Training Loss on iteration  2 209045761481203.38\n","Training Loss on iteration  3 209045761481168.75\n","Training Loss on iteration  4 209045761481134.12\n","Training Loss on iteration  5 209045761481099.5\n","Training Loss on iteration  6 209045761481064.9\n","Training Loss on iteration  7 209045761481030.38\n","Training Loss on iteration  8 209045761480995.8\n","Training Loss on iteration  9 209045761480961.28\n","Training Loss on iteration  10 209045761480926.78\n","Training Loss on iteration  11 209045761480892.34\n","Training Loss on iteration  12 209045761480857.88\n","Training Loss on iteration  13 209045761480823.47\n","Training Loss on iteration  14 209045761480789.06\n","Training Loss on iteration  15 209045761480754.7\n","Training Loss on iteration  16 209045761480720.3\n","Training Loss on iteration  17 209045761480686.03\n","Training Loss on iteration  18 209045761480651.7\n","Training Loss on iteration  19 209045761480617.44\n","Training Loss on iteration  20 209045761480583.16\n","Training Loss on iteration  21 209045761480548.94\n","Training Loss on iteration  22 209045761480514.72\n","Training Loss on iteration  23 209045761480480.53\n","Training Loss on iteration  24 209045761480446.38\n","Training Loss on iteration  25 209045761480412.22\n","Training Loss on iteration  26 209045761480378.16\n","Training Loss on iteration  27 209045761480344.0\n","Training Loss on iteration  28 209045761480309.97\n","Training Loss on iteration  29 209045761480275.9\n","Training Loss on iteration  30 209045761480241.9\n","Training Loss on iteration  31 209045761480207.9\n","Training Loss on iteration  32 209045761480173.94\n","Training Loss on iteration  33 209045761480140.0\n","Training Loss on iteration  34 209045761480106.06\n","Training Loss on iteration  35 209045761480072.16\n","Training Loss on iteration  36 209045761480038.28\n","Training Loss on iteration  37 209045761480004.4\n","Training Loss on iteration  38 209045761479970.56\n","Training Loss on iteration  39 209045761479936.78\n","Training Loss on iteration  40 209045761479903.0\n","Training Loss on iteration  41 209045761479869.25\n","Training Loss on iteration  42 209045761479835.47\n","Training Loss on iteration  43 209045761479801.78\n","Training Loss on iteration  44 209045761479768.1\n","Training Loss on iteration  45 209045761479734.44\n","Training Loss on iteration  46 209045761479700.78\n","Training Loss on iteration  47 209045761479667.16\n","Training Loss on iteration  48 209045761479633.53\n","Training Loss on iteration  49 209045761479599.97\n","Training Loss on iteration  50 209045761479566.44\n","Training Loss on iteration  51 209045761479532.88\n","Training Loss on iteration  52 209045761479499.38\n","Training Loss on iteration  53 209045761479465.9\n","Training Loss on iteration  54 209045761479432.44\n","Training Loss on iteration  55 209045761479398.97\n","Training Loss on iteration  56 209045761479365.6\n","Training Loss on iteration  57 209045761479332.2\n","Training Loss on iteration  58 209045761479298.84\n","Training Loss on iteration  59 209045761479265.5\n","Training Loss on iteration  60 209045761479232.16\n","Training Loss on iteration  61 209045761479198.84\n","Training Loss on iteration  62 209045761479165.6\n","Training Loss on iteration  63 209045761479132.34\n","Training Loss on iteration  64 209045761479099.1\n","Training Loss on iteration  65 209045761479065.9\n","Training Loss on iteration  66 209045761479032.72\n","Training Loss on iteration  67 209045761478999.56\n","Training Loss on iteration  68 209045761478966.44\n","Training Loss on iteration  69 209045761478933.3\n","Training Loss on iteration  70 209045761478900.22\n","Training Loss on iteration  71 209045761478867.12\n","Training Loss on iteration  72 209045761478834.12\n","Training Loss on iteration  73 209045761478801.1\n","Training Loss on iteration  74 209045761478768.1\n","Training Loss on iteration  75 209045761478735.1\n","Training Loss on iteration  76 209045761478702.12\n","Training Loss on iteration  77 209045761478669.22\n","Training Loss on iteration  78 209045761478636.34\n","Training Loss on iteration  79 209045761478603.38\n","Training Loss on iteration  80 209045761478570.56\n","Training Loss on iteration  81 209045761478537.72\n","Training Loss on iteration  82 209045761478504.88\n","Training Loss on iteration  83 209045761478472.1\n","Training Loss on iteration  84 209045761478439.34\n","Training Loss on iteration  85 209045761478406.56\n","Training Loss on iteration  86 209045761478373.88\n","Training Loss on iteration  87 209045761478341.16\n","Training Loss on iteration  88 209045761478308.47\n","Training Loss on iteration  89 209045761478275.8\n","Training Loss on iteration  90 209045761478243.2\n","Training Loss on iteration  91 209045761478210.56\n","Training Loss on iteration  92 209045761478177.97\n","Training Loss on iteration  93 209045761478145.4\n","Training Loss on iteration  94 209045761478112.84\n","Training Loss on iteration  95 209045761478080.34\n","Training Loss on iteration  96 209045761478047.8\n","Training Loss on iteration  97 209045761478015.38\n","Training Loss on iteration  98 209045761477982.9\n","Training Loss on iteration  99 209045761477950.44\n","Training Loss on iteration  100 209045761477918.06\n","Training Loss on iteration  101 209045761477885.66\n","Training Loss on iteration  102 209045761477853.3\n","Training Loss on iteration  103 209045761477820.94\n","Training Loss on iteration  104 209045761477788.62\n","Training Loss on iteration  105 209045761477756.3\n","Training Loss on iteration  106 209045761477724.0\n","Training Loss on iteration  107 209045761477691.8\n","Training Loss on iteration  108 209045761477659.6\n","Training Loss on iteration  109 209045761477627.34\n","Training Loss on iteration  110 209045761477595.16\n","Training Loss on iteration  111 209045761477563.0\n","Training Loss on iteration  112 209045761477530.84\n","Training Loss on iteration  113 209045761477498.72\n","Training Loss on iteration  114 209045761477466.62\n","Training Loss on iteration  115 209045761477434.56\n","Training Loss on iteration  116 209045761477402.5\n","Training Loss on iteration  117 209045761477370.44\n","Training Loss on iteration  118 209045761477338.47\n","Training Loss on iteration  119 209045761477306.47\n","Training Loss on iteration  120 209045761477274.5\n","Training Loss on iteration  121 209045761477242.6\n","Training Loss on iteration  122 209045761477210.66\n","Training Loss on iteration  123 209045761477178.75\n","Training Loss on iteration  124 209045761477146.84\n","Training Loss on iteration  125 209045761477115.0\n","Training Loss on iteration  126 209045761477083.2\n","Training Loss on iteration  127 209045761477051.34\n","Training Loss on iteration  128 209045761477019.6\n","Training Loss on iteration  129 209045761476987.78\n","Training Loss on iteration  130 209045761476956.1\n","Training Loss on iteration  131 209045761476924.34\n","Training Loss on iteration  132 209045761476892.66\n","Training Loss on iteration  133 209045761476860.97\n","Training Loss on iteration  134 209045761476829.3\n","Training Loss on iteration  135 209045761476797.66\n","Training Loss on iteration  136 209045761476766.06\n","Training Loss on iteration  137 209045761476734.44\n","Training Loss on iteration  138 209045761476702.9\n","Training Loss on iteration  139 209045761476671.34\n","Training Loss on iteration  140 209045761476639.78\n","Training Loss on iteration  141 209045761476608.3\n","Training Loss on iteration  142 209045761476576.8\n","Training Loss on iteration  143 209045761476545.34\n","Training Loss on iteration  144 209045761476513.88\n","Training Loss on iteration  145 209045761476482.5\n","Training Loss on iteration  146 209045761476451.1\n","Training Loss on iteration  147 209045761476419.75\n","Training Loss on iteration  148 209045761476388.38\n","Training Loss on iteration  149 209045761476357.06\n","Training Loss on iteration  150 209045761476325.75\n","Training Loss on iteration  151 209045761476294.44\n","Training Loss on iteration  152 209045761476263.2\n","Training Loss on iteration  153 209045761476231.97\n","Training Loss on iteration  154 209045761476200.7\n","Training Loss on iteration  155 209045761476169.5\n","Training Loss on iteration  156 209045761476138.34\n","Training Loss on iteration  157 209045761476107.2\n","Training Loss on iteration  158 209045761476076.03\n","Training Loss on iteration  159 209045761476044.9\n","Training Loss on iteration  160 209045761476013.84\n","Training Loss on iteration  161 209045761475982.78\n","Training Loss on iteration  162 209045761475951.72\n","Training Loss on iteration  163 209045761475920.72\n","Training Loss on iteration  164 209045761475889.66\n","Training Loss on iteration  165 209045761475858.7\n","Training Loss on iteration  166 209045761475827.72\n","Training Loss on iteration  167 209045761475796.78\n","Training Loss on iteration  168 209045761475765.88\n","Training Loss on iteration  169 209045761475734.97\n","Training Loss on iteration  170 209045761475704.06\n","Training Loss on iteration  171 209045761475673.25\n","Training Loss on iteration  172 209045761475642.4\n","Training Loss on iteration  173 209045761475611.6\n","Training Loss on iteration  174 209045761475580.78\n","Training Loss on iteration  175 209045761475550.0\n","Training Loss on iteration  176 209045761475519.25\n","Training Loss on iteration  177 209045761475488.5\n","Training Loss on iteration  178 209045761475457.84\n","Training Loss on iteration  179 209045761475427.12\n","Training Loss on iteration  180 209045761475396.5\n","Training Loss on iteration  181 209045761475365.84\n","Training Loss on iteration  182 209045761475335.2\n","Training Loss on iteration  183 209045761475304.62\n","Training Loss on iteration  184 209045761475274.0\n","Training Loss on iteration  185 209045761475243.44\n","Training Loss on iteration  186 209045761475212.9\n","Training Loss on iteration  187 209045761475182.4\n","Training Loss on iteration  188 209045761475151.88\n","Training Loss on iteration  189 209045761475121.44\n","Training Loss on iteration  190 209045761475090.97\n","Training Loss on iteration  191 209045761475060.5\n","Training Loss on iteration  192 209045761475030.1\n","Training Loss on iteration  193 209045761474999.75\n","Training Loss on iteration  194 209045761474969.34\n","Training Loss on iteration  195 209045761474939.03\n","Training Loss on iteration  196 209045761474908.7\n","Training Loss on iteration  197 209045761474878.38\n","Training Loss on iteration  198 209045761474848.1\n","Training Loss on iteration  199 209045761474817.8\n","Training Loss on iteration  200 209045761474787.6\n","Training Loss on iteration  201 209045761474757.34\n","Training Loss on iteration  202 209045761474727.12\n","Training Loss on iteration  203 209045761474696.97\n","Training Loss on iteration  204 209045761474666.8\n","Training Loss on iteration  205 209045761474636.7\n","Training Loss on iteration  206 209045761474606.56\n","Training Loss on iteration  207 209045761474576.44\n","Training Loss on iteration  208 209045761474546.38\n","Training Loss on iteration  209 209045761474516.34\n","Training Loss on iteration  210 209045761474486.28\n","Training Loss on iteration  211 209045761474456.25\n","Training Loss on iteration  212 209045761474426.3\n","Training Loss on iteration  213 209045761474396.28\n","Training Loss on iteration  214 209045761474366.34\n","Training Loss on iteration  215 209045761474336.4\n","Training Loss on iteration  216 209045761474306.5\n","Training Loss on iteration  217 209045761474276.62\n","Training Loss on iteration  218 209045761474246.78\n","Training Loss on iteration  219 209045761474216.9\n","Training Loss on iteration  220 209045761474187.06\n","Training Loss on iteration  221 209045761474157.28\n","Training Loss on iteration  222 209045761474127.47\n","Training Loss on iteration  223 209045761474097.72\n","Training Loss on iteration  224 209045761474067.97\n","Training Loss on iteration  225 209045761474038.25\n","Training Loss on iteration  226 209045761474008.5\n","Training Loss on iteration  227 209045761473978.8\n","Training Loss on iteration  228 209045761473949.22\n","Training Loss on iteration  229 209045761473919.53\n","Training Loss on iteration  230 209045761473889.9\n","Training Loss on iteration  231 209045761473860.3\n","Training Loss on iteration  232 209045761473830.72\n","Training Loss on iteration  233 209045761473801.16\n","Training Loss on iteration  234 209045761473771.6\n","Training Loss on iteration  235 209045761473742.12\n","Training Loss on iteration  236 209045761473712.62\n","Training Loss on iteration  237 209045761473683.12\n","Training Loss on iteration  238 209045761473653.66\n","Training Loss on iteration  239 209045761473624.22\n","Training Loss on iteration  240 209045761473594.78\n","Training Loss on iteration  241 209045761473565.44\n","Training Loss on iteration  242 209045761473536.03\n","Training Loss on iteration  243 209045761473506.7\n","Training Loss on iteration  244 209045761473477.34\n","Training Loss on iteration  245 209045761473448.03\n","Training Loss on iteration  246 209045761473418.75\n","Training Loss on iteration  247 209045761473389.44\n","Training Loss on iteration  248 209045761473360.16\n","Training Loss on iteration  249 209045761473330.94\n","Training Loss on iteration  250 209045761473301.75\n","Training Loss on iteration  251 209045761473272.53\n","Training Loss on iteration  252 209045761473243.38\n","Training Loss on iteration  253 209045761473214.22\n","Training Loss on iteration  254 209045761473185.06\n","Training Loss on iteration  255 209045761473155.97\n","Training Loss on iteration  256 209045761473126.84\n","Training Loss on iteration  257 209045761473097.8\n","Training Loss on iteration  258 209045761473068.72\n","Training Loss on iteration  259 209045761473039.7\n","Training Loss on iteration  260 209045761473010.66\n","Training Loss on iteration  261 209045761472981.7\n","Training Loss on iteration  262 209045761472952.72\n","Training Loss on iteration  263 209045761472923.75\n","Training Loss on iteration  264 209045761472894.84\n","Training Loss on iteration  265 209045761472865.9\n","Training Loss on iteration  266 209045761472837.03\n","Training Loss on iteration  267 209045761472808.12\n","Training Loss on iteration  268 209045761472779.28\n","Training Loss on iteration  269 209045761472750.4\n","Training Loss on iteration  270 209045761472721.66\n","Training Loss on iteration  271 209045761472692.8\n","Training Loss on iteration  272 209045761472664.06\n","Training Loss on iteration  273 209045761472635.3\n","Training Loss on iteration  274 209045761472606.56\n","Training Loss on iteration  275 209045761472577.88\n","Training Loss on iteration  276 209045761472549.2\n","Training Loss on iteration  277 209045761472520.53\n","Training Loss on iteration  278 209045761472491.84\n","Training Loss on iteration  279 209045761472463.22\n","Training Loss on iteration  280 209045761472434.6\n","Training Loss on iteration  281 209045761472406.0\n","Training Loss on iteration  282 209045761472377.4\n","Training Loss on iteration  283 209045761472348.88\n","Training Loss on iteration  284 209045761472320.3\n","Training Loss on iteration  285 209045761472291.84\n","Training Loss on iteration  286 209045761472263.28\n","Training Loss on iteration  287 209045761472234.8\n","Training Loss on iteration  288 209045761472206.4\n","Training Loss on iteration  289 209045761472177.94\n","Training Loss on iteration  290 209045761472149.53\n","Training Loss on iteration  291 209045761472121.16\n","Training Loss on iteration  292 209045761472092.72\n","Training Loss on iteration  293 209045761472064.38\n","Training Loss on iteration  294 209045761472036.03\n","Training Loss on iteration  295 209045761472007.72\n","Training Loss on iteration  296 209045761471979.38\n","Training Loss on iteration  297 209045761471951.16\n","Training Loss on iteration  298 209045761471922.9\n","Training Loss on iteration  299 209045761471894.62\n","Training Loss on iteration  300 209045761471866.4\n","Training Loss on iteration  301 209045761471838.22\n","Training Loss on iteration  302 209045761471810.0\n","Training Loss on iteration  303 209045761471781.88\n","Training Loss on iteration  304 209045761471753.72\n","Training Loss on iteration  305 209045761471725.56\n","Training Loss on iteration  306 209045761471697.53\n","Training Loss on iteration  307 209045761471669.4\n","Training Loss on iteration  308 209045761471641.34\n","Training Loss on iteration  309 209045761471613.3\n","Training Loss on iteration  310 209045761471585.25\n","Training Loss on iteration  311 209045761471557.28\n","Training Loss on iteration  312 209045761471529.25\n","Training Loss on iteration  313 209045761471501.3\n","Training Loss on iteration  314 209045761471473.34\n","Training Loss on iteration  315 209045761471445.4\n","Training Loss on iteration  316 209045761471417.53\n","Training Loss on iteration  317 209045761471389.6\n","Training Loss on iteration  318 209045761471361.75\n","Training Loss on iteration  319 209045761471333.9\n","Training Loss on iteration  320 209045761471306.06\n","Training Loss on iteration  321 209045761471278.25\n","Training Loss on iteration  322 209045761471250.44\n","Training Loss on iteration  323 209045761471222.7\n","Training Loss on iteration  324 209045761471194.94\n","Training Loss on iteration  325 209045761471167.12\n","Training Loss on iteration  326 209045761471139.47\n","Training Loss on iteration  327 209045761471111.75\n","Training Loss on iteration  328 209045761471084.06\n","Training Loss on iteration  329 209045761471056.38\n","Training Loss on iteration  330 209045761471028.78\n","Training Loss on iteration  331 209045761471001.12\n","Training Loss on iteration  332 209045761470973.53\n","Training Loss on iteration  333 209045761470945.94\n","Training Loss on iteration  334 209045761470918.4\n","Training Loss on iteration  335 209045761470890.84\n","Training Loss on iteration  336 209045761470863.3\n","Training Loss on iteration  337 209045761470835.78\n","Training Loss on iteration  338 209045761470808.28\n","Training Loss on iteration  339 209045761470780.84\n","Training Loss on iteration  340 209045761470753.4\n","Training Loss on iteration  341 209045761470725.97\n","Training Loss on iteration  342 209045761470698.56\n","Training Loss on iteration  343 209045761470671.16\n","Training Loss on iteration  344 209045761470643.75\n","Training Loss on iteration  345 209045761470616.4\n","Training Loss on iteration  346 209045761470589.06\n","Training Loss on iteration  347 209045761470561.75\n","Training Loss on iteration  348 209045761470534.44\n","Training Loss on iteration  349 209045761470507.2\n","Training Loss on iteration  350 209045761470479.94\n","Training Loss on iteration  351 209045761470452.66\n","Training Loss on iteration  352 209045761470425.47\n","Training Loss on iteration  353 209045761470398.25\n","Training Loss on iteration  354 209045761470371.0\n","Training Loss on iteration  355 209045761470343.88\n","Training Loss on iteration  356 209045761470316.75\n","Training Loss on iteration  357 209045761470289.6\n","Training Loss on iteration  358 209045761470262.47\n","Training Loss on iteration  359 209045761470235.4\n","Training Loss on iteration  360 209045761470208.3\n","Training Loss on iteration  361 209045761470181.25\n","Training Loss on iteration  362 209045761470154.25\n","Training Loss on iteration  363 209045761470127.2\n","Training Loss on iteration  364 209045761470100.22\n","Training Loss on iteration  365 209045761470073.22\n","Training Loss on iteration  366 209045761470046.28\n","Training Loss on iteration  367 209045761470019.34\n","Training Loss on iteration  368 209045761469992.38\n","Training Loss on iteration  369 209045761469965.47\n","Training Loss on iteration  370 209045761469938.6\n","Training Loss on iteration  371 209045761469911.72\n","Training Loss on iteration  372 209045761469884.9\n","Training Loss on iteration  373 209045761469858.06\n","Training Loss on iteration  374 209045761469831.25\n","Training Loss on iteration  375 209045761469804.44\n","Training Loss on iteration  376 209045761469777.7\n","Training Loss on iteration  377 209045761469750.9\n","Training Loss on iteration  378 209045761469724.16\n","Training Loss on iteration  379 209045761469697.47\n","Training Loss on iteration  380 209045761469670.75\n","Training Loss on iteration  381 209045761469644.1\n","Training Loss on iteration  382 209045761469617.4\n","Training Loss on iteration  383 209045761469590.72\n","Training Loss on iteration  384 209045761469564.12\n","Training Loss on iteration  385 209045761469537.53\n","Training Loss on iteration  386 209045761469510.9\n","Training Loss on iteration  387 209045761469484.34\n","Training Loss on iteration  388 209045761469457.78\n","Training Loss on iteration  389 209045761469431.22\n","Training Loss on iteration  390 209045761469404.75\n","Training Loss on iteration  391 209045761469378.22\n","Training Loss on iteration  392 209045761469351.75\n","Training Loss on iteration  393 209045761469325.3\n","Training Loss on iteration  394 209045761469298.84\n","Training Loss on iteration  395 209045761469272.44\n","Training Loss on iteration  396 209045761469246.0\n","Training Loss on iteration  397 209045761469219.6\n","Training Loss on iteration  398 209045761469193.22\n","Training Loss on iteration  399 209045761469166.9\n","Training Loss on iteration  400 209045761469140.56\n","Training Loss on iteration  401 209045761469114.22\n","Training Loss on iteration  402 209045761469087.94\n","Training Loss on iteration  403 209045761469061.66\n","Training Loss on iteration  404 209045761469035.38\n","Training Loss on iteration  405 209045761469009.12\n","Training Loss on iteration  406 209045761468982.9\n","Training Loss on iteration  407 209045761468956.75\n","Training Loss on iteration  408 209045761468930.53\n","Training Loss on iteration  409 209045761468904.38\n","Training Loss on iteration  410 209045761468878.2\n","Training Loss on iteration  411 209045761468852.06\n","Training Loss on iteration  412 209045761468825.94\n","Training Loss on iteration  413 209045761468799.84\n","Training Loss on iteration  414 209045761468773.75\n","Training Loss on iteration  415 209045761468747.7\n","Training Loss on iteration  416 209045761468721.66\n","Training Loss on iteration  417 209045761468695.6\n","Training Loss on iteration  418 209045761468669.62\n","Training Loss on iteration  419 209045761468643.62\n","Training Loss on iteration  420 209045761468617.66\n","Training Loss on iteration  421 209045761468591.7\n","Training Loss on iteration  422 209045761468565.8\n","Training Loss on iteration  423 209045761468539.84\n","Training Loss on iteration  424 209045761468513.94\n","Training Loss on iteration  425 209045761468488.06\n","Training Loss on iteration  426 209045761468462.2\n","Training Loss on iteration  427 209045761468436.3\n","Training Loss on iteration  428 209045761468410.5\n","Training Loss on iteration  429 209045761468384.7\n","Training Loss on iteration  430 209045761468358.9\n","Training Loss on iteration  431 209045761468333.1\n","Training Loss on iteration  432 209045761468307.38\n","Training Loss on iteration  433 209045761468281.66\n","Training Loss on iteration  434 209045761468255.9\n","Training Loss on iteration  435 209045761468230.22\n","Training Loss on iteration  436 209045761468204.5\n","Training Loss on iteration  437 209045761468178.8\n","Training Loss on iteration  438 209045761468153.2\n","Training Loss on iteration  439 209045761468127.53\n","Training Loss on iteration  440 209045761468101.94\n","Training Loss on iteration  441 209045761468076.3\n","Training Loss on iteration  442 209045761468050.78\n","Training Loss on iteration  443 209045761468025.2\n","Training Loss on iteration  444 209045761467999.66\n","Training Loss on iteration  445 209045761467974.12\n","Training Loss on iteration  446 209045761467948.6\n","Training Loss on iteration  447 209045761467923.1\n","Training Loss on iteration  448 209045761467897.66\n","Training Loss on iteration  449 209045761467872.2\n","Training Loss on iteration  450 209045761467846.75\n","Training Loss on iteration  451 209045761467821.3\n","Training Loss on iteration  452 209045761467795.94\n","Training Loss on iteration  453 209045761467770.53\n","Training Loss on iteration  454 209045761467745.2\n","Training Loss on iteration  455 209045761467719.8\n","Training Loss on iteration  456 209045761467694.47\n","Training Loss on iteration  457 209045761467669.16\n","Training Loss on iteration  458 209045761467643.84\n","Training Loss on iteration  459 209045761467618.56\n","Training Loss on iteration  460 209045761467593.3\n","Training Loss on iteration  461 209045761467568.03\n","Training Loss on iteration  462 209045761467542.88\n","Training Loss on iteration  463 209045761467517.62\n","Training Loss on iteration  464 209045761467492.4\n","Training Loss on iteration  465 209045761467467.25\n","Training Loss on iteration  466 209045761467442.06\n","Training Loss on iteration  467 209045761467416.97\n","Training Loss on iteration  468 209045761467391.8\n","Training Loss on iteration  469 209045761467366.75\n","Training Loss on iteration  470 209045761467341.6\n","Training Loss on iteration  471 209045761467316.56\n","Training Loss on iteration  472 209045761467291.5\n","Training Loss on iteration  473 209045761467266.47\n","Training Loss on iteration  474 209045761467241.47\n","Training Loss on iteration  475 209045761467216.44\n","Training Loss on iteration  476 209045761467191.47\n","Training Loss on iteration  477 209045761467166.5\n","Training Loss on iteration  478 209045761467141.53\n","Training Loss on iteration  479 209045761467116.62\n","Training Loss on iteration  480 209045761467091.66\n","Training Loss on iteration  481 209045761467066.75\n","Training Loss on iteration  482 209045761467041.9\n","Training Loss on iteration  483 209045761467017.0\n","Training Loss on iteration  484 209045761466992.16\n","Training Loss on iteration  485 209045761466967.3\n","Training Loss on iteration  486 209045761466942.56\n","Training Loss on iteration  487 209045761466917.78\n","Training Loss on iteration  488 209045761466892.94\n","Training Loss on iteration  489 209045761466868.22\n","Training Loss on iteration  490 209045761466843.47\n","Training Loss on iteration  491 209045761466818.72\n","Training Loss on iteration  492 209045761466794.06\n","Training Loss on iteration  493 209045761466769.34\n","Training Loss on iteration  494 209045761466744.62\n","Training Loss on iteration  495 209045761466720.0\n","Training Loss on iteration  496 209045761466695.34\n","Training Loss on iteration  497 209045761466670.72\n","Training Loss on iteration  498 209045761466646.12\n","Training Loss on iteration  499 209045761466621.53\n","Training Loss on iteration  500 209045761466596.9\n","Training Loss on iteration  501 209045761466572.4\n","Training Loss on iteration  502 209045761466547.84\n","Training Loss on iteration  503 209045761466523.3\n","Training Loss on iteration  504 209045761466498.84\n","Training Loss on iteration  505 209045761466474.34\n","Training Loss on iteration  506 209045761466449.84\n","Training Loss on iteration  507 209045761466425.4\n","Training Loss on iteration  508 209045761466400.97\n","Training Loss on iteration  509 209045761466376.53\n","Training Loss on iteration  510 209045761466352.12\n","Training Loss on iteration  511 209045761466327.72\n","Training Loss on iteration  512 209045761466303.34\n","Training Loss on iteration  513 209045761466279.0\n","Training Loss on iteration  514 209045761466254.7\n","Training Loss on iteration  515 209045761466230.38\n","Training Loss on iteration  516 209045761466206.06\n","Training Loss on iteration  517 209045761466181.75\n","Training Loss on iteration  518 209045761466157.5\n","Training Loss on iteration  519 209045761466133.25\n","Training Loss on iteration  520 209045761466109.0\n","Training Loss on iteration  521 209045761466084.78\n","Training Loss on iteration  522 209045761466060.56\n","Training Loss on iteration  523 209045761466036.38\n","Training Loss on iteration  524 209045761466012.22\n","Training Loss on iteration  525 209045761465988.1\n","Training Loss on iteration  526 209045761465963.94\n","Training Loss on iteration  527 209045761465939.8\n","Training Loss on iteration  528 209045761465915.72\n","Training Loss on iteration  529 209045761465891.62\n","Training Loss on iteration  530 209045761465867.56\n","Training Loss on iteration  531 209045761465843.47\n","Training Loss on iteration  532 209045761465819.47\n","Training Loss on iteration  533 209045761465795.4\n","Training Loss on iteration  534 209045761465771.44\n","Training Loss on iteration  535 209045761465747.47\n","Training Loss on iteration  536 209045761465723.5\n","Training Loss on iteration  537 209045761465699.53\n","Training Loss on iteration  538 209045761465675.62\n","Training Loss on iteration  539 209045761465651.7\n","Training Loss on iteration  540 209045761465627.78\n","Training Loss on iteration  541 209045761465603.88\n","Training Loss on iteration  542 209045761465580.03\n","Training Loss on iteration  543 209045761465556.12\n","Training Loss on iteration  544 209045761465532.3\n","Training Loss on iteration  545 209045761465508.5\n","Training Loss on iteration  546 209045761465484.72\n","Training Loss on iteration  547 209045761465460.9\n","Training Loss on iteration  548 209045761465437.12\n","Training Loss on iteration  549 209045761465413.4\n","Training Loss on iteration  550 209045761465389.62\n","Training Loss on iteration  551 209045761465365.88\n","Training Loss on iteration  552 209045761465342.22\n","Training Loss on iteration  553 209045761465318.5\n","Training Loss on iteration  554 209045761465294.84\n","Training Loss on iteration  555 209045761465271.2\n","Training Loss on iteration  556 209045761465247.53\n","Training Loss on iteration  557 209045761465223.9\n","Training Loss on iteration  558 209045761465200.28\n","Training Loss on iteration  559 209045761465176.75\n","Training Loss on iteration  560 209045761465153.12\n","Training Loss on iteration  561 209045761465129.6\n","Training Loss on iteration  562 209045761465106.03\n","Training Loss on iteration  563 209045761465082.5\n","Training Loss on iteration  564 209045761465059.03\n","Training Loss on iteration  565 209045761465035.53\n","Training Loss on iteration  566 209045761465012.06\n","Training Loss on iteration  567 209045761464988.56\n","Training Loss on iteration  568 209045761464965.12\n","Training Loss on iteration  569 209045761464941.72\n","Training Loss on iteration  570 209045761464918.28\n","Training Loss on iteration  571 209045761464894.88\n","Training Loss on iteration  572 209045761464871.53\n","Training Loss on iteration  573 209045761464848.12\n","Training Loss on iteration  574 209045761464824.8\n","Training Loss on iteration  575 209045761464801.47\n","Training Loss on iteration  576 209045761464778.16\n","Training Loss on iteration  577 209045761464754.84\n","Training Loss on iteration  578 209045761464731.56\n","Training Loss on iteration  579 209045761464708.3\n","Training Loss on iteration  580 209045761464685.03\n","Training Loss on iteration  581 209045761464661.8\n","Training Loss on iteration  582 209045761464638.6\n","Training Loss on iteration  583 209045761464615.38\n","Training Loss on iteration  584 209045761464592.22\n","Training Loss on iteration  585 209045761464569.0\n","Training Loss on iteration  586 209045761464545.88\n","Training Loss on iteration  587 209045761464522.75\n","Training Loss on iteration  588 209045761464499.6\n","Training Loss on iteration  589 209045761464476.5\n","Training Loss on iteration  590 209045761464453.4\n","Training Loss on iteration  591 209045761464430.34\n","Training Loss on iteration  592 209045761464407.28\n","Training Loss on iteration  593 209045761464384.22\n","Training Loss on iteration  594 209045761464361.2\n","Training Loss on iteration  595 209045761464338.2\n","Training Loss on iteration  596 209045761464315.22\n","Training Loss on iteration  597 209045761464292.22\n","Training Loss on iteration  598 209045761464269.28\n","Training Loss on iteration  599 209045761464246.3\n","Training Loss on iteration  600 209045761464223.4\n","Training Loss on iteration  601 209045761464200.47\n","Training Loss on iteration  602 209045761464177.6\n","Training Loss on iteration  603 209045761464154.7\n","Training Loss on iteration  604 209045761464131.84\n","Training Loss on iteration  605 209045761464108.94\n","Training Loss on iteration  606 209045761464086.1\n","Training Loss on iteration  607 209045761464063.34\n","Training Loss on iteration  608 209045761464040.5\n","Training Loss on iteration  609 209045761464017.72\n","Training Loss on iteration  610 209045761463994.94\n","Training Loss on iteration  611 209045761463972.16\n","Training Loss on iteration  612 209045761463949.44\n","Training Loss on iteration  613 209045761463926.72\n","Training Loss on iteration  614 209045761463903.97\n","Training Loss on iteration  615 209045761463881.3\n","Training Loss on iteration  616 209045761463858.62\n","Training Loss on iteration  617 209045761463835.94\n","Training Loss on iteration  618 209045761463813.34\n","Training Loss on iteration  619 209045761463790.66\n","Training Loss on iteration  620 209045761463768.06\n","Training Loss on iteration  621 209045761463745.47\n","Training Loss on iteration  622 209045761463722.88\n","Training Loss on iteration  623 209045761463700.3\n","Training Loss on iteration  624 209045761463677.75\n","Training Loss on iteration  625 209045761463655.2\n","Training Loss on iteration  626 209045761463632.66\n","Training Loss on iteration  627 209045761463610.22\n","Training Loss on iteration  628 209045761463587.7\n","Training Loss on iteration  629 209045761463565.22\n","Training Loss on iteration  630 209045761463542.78\n","Training Loss on iteration  631 209045761463520.34\n","Training Loss on iteration  632 209045761463497.88\n","Training Loss on iteration  633 209045761463475.47\n","Training Loss on iteration  634 209045761463453.1\n","Training Loss on iteration  635 209045761463430.72\n","Training Loss on iteration  636 209045761463408.34\n","Training Loss on iteration  637 209045761463385.97\n","Training Loss on iteration  638 209045761463363.62\n","Training Loss on iteration  639 209045761463341.34\n","Training Loss on iteration  640 209045761463319.0\n","Training Loss on iteration  641 209045761463296.72\n","Training Loss on iteration  642 209045761463274.44\n","Training Loss on iteration  643 209045761463252.22\n","Training Loss on iteration  644 209045761463229.94\n","Training Loss on iteration  645 209045761463207.72\n","Training Loss on iteration  646 209045761463185.5\n","Training Loss on iteration  647 209045761463163.28\n","Training Loss on iteration  648 209045761463141.12\n","Training Loss on iteration  649 209045761463118.97\n","Training Loss on iteration  650 209045761463096.8\n","Training Loss on iteration  651 209045761463074.66\n","Training Loss on iteration  652 209045761463052.53\n","Training Loss on iteration  653 209045761463030.44\n","Training Loss on iteration  654 209045761463008.38\n","Training Loss on iteration  655 209045761462986.28\n","Training Loss on iteration  656 209045761462964.2\n","Training Loss on iteration  657 209045761462942.16\n","Training Loss on iteration  658 209045761462920.16\n","Training Loss on iteration  659 209045761462898.12\n","Training Loss on iteration  660 209045761462876.16\n","Training Loss on iteration  661 209045761462854.16\n","Training Loss on iteration  662 209045761462832.16\n","Training Loss on iteration  663 209045761462810.25\n","Training Loss on iteration  664 209045761462788.28\n","Training Loss on iteration  665 209045761462766.34\n","Training Loss on iteration  666 209045761462744.44\n","Training Loss on iteration  667 209045761462722.56\n","Training Loss on iteration  668 209045761462700.66\n","Training Loss on iteration  669 209045761462678.78\n","Training Loss on iteration  670 209045761462656.97\n","Training Loss on iteration  671 209045761462635.12\n","Training Loss on iteration  672 209045761462613.3\n","Training Loss on iteration  673 209045761462591.53\n","Training Loss on iteration  674 209045761462569.72\n","Training Loss on iteration  675 209045761462547.97\n","Training Loss on iteration  676 209045761462526.22\n","Training Loss on iteration  677 209045761462504.47\n","Training Loss on iteration  678 209045761462482.72\n","Training Loss on iteration  679 209045761462461.0\n","Training Loss on iteration  680 209045761462439.34\n","Training Loss on iteration  681 209045761462417.66\n","Training Loss on iteration  682 209045761462396.0\n","Training Loss on iteration  683 209045761462374.34\n","Training Loss on iteration  684 209045761462352.7\n","Training Loss on iteration  685 209045761462331.1\n","Training Loss on iteration  686 209045761462309.5\n","Training Loss on iteration  687 209045761462287.88\n","Training Loss on iteration  688 209045761462266.3\n","Training Loss on iteration  689 209045761462244.75\n","Training Loss on iteration  690 209045761462223.2\n","Training Loss on iteration  691 209045761462201.66\n","Training Loss on iteration  692 209045761462180.16\n","Training Loss on iteration  693 209045761462158.62\n","Training Loss on iteration  694 209045761462137.12\n","Training Loss on iteration  695 209045761462115.66\n","Training Loss on iteration  696 209045761462094.2\n","Training Loss on iteration  697 209045761462072.78\n","Training Loss on iteration  698 209045761462051.34\n","Training Loss on iteration  699 209045761462029.9\n","Training Loss on iteration  700 209045761462008.53\n","Training Loss on iteration  701 209045761461987.16\n","Training Loss on iteration  702 209045761461965.78\n","Training Loss on iteration  703 209045761461944.4\n","Training Loss on iteration  704 209045761461923.1\n","Training Loss on iteration  705 209045761461901.75\n","Training Loss on iteration  706 209045761461880.44\n","Training Loss on iteration  707 209045761461859.16\n","Training Loss on iteration  708 209045761461837.84\n","Training Loss on iteration  709 209045761461816.6\n","Training Loss on iteration  710 209045761461795.34\n","Training Loss on iteration  711 209045761461774.16\n","Training Loss on iteration  712 209045761461752.88\n","Training Loss on iteration  713 209045761461731.72\n","Training Loss on iteration  714 209045761461710.5\n","Training Loss on iteration  715 209045761461689.3\n","Training Loss on iteration  716 209045761461668.16\n","Training Loss on iteration  717 209045761461647.06\n","Training Loss on iteration  718 209045761461625.88\n","Training Loss on iteration  719 209045761461604.75\n","Training Loss on iteration  720 209045761461583.7\n","Training Loss on iteration  721 209045761461562.56\n","Training Loss on iteration  722 209045761461541.47\n","Training Loss on iteration  723 209045761461520.44\n","Training Loss on iteration  724 209045761461499.4\n","Training Loss on iteration  725 209045761461478.34\n","Training Loss on iteration  726 209045761461457.34\n","Training Loss on iteration  727 209045761461436.34\n","Training Loss on iteration  728 209045761461415.34\n","Training Loss on iteration  729 209045761461394.38\n","Training Loss on iteration  730 209045761461373.4\n","Training Loss on iteration  731 209045761461352.47\n","Training Loss on iteration  732 209045761461331.56\n","Training Loss on iteration  733 209045761461310.66\n","Training Loss on iteration  734 209045761461289.72\n","Training Loss on iteration  735 209045761461268.84\n","Training Loss on iteration  736 209045761461247.94\n","Training Loss on iteration  737 209045761461227.1\n","Training Loss on iteration  738 209045761461206.28\n","Training Loss on iteration  739 209045761461185.4\n","Training Loss on iteration  740 209045761461164.62\n","Training Loss on iteration  741 209045761461143.88\n","Training Loss on iteration  742 209045761461123.03\n","Training Loss on iteration  743 209045761461102.25\n","Training Loss on iteration  744 209045761461081.5\n","Training Loss on iteration  745 209045761461060.75\n","Training Loss on iteration  746 209045761461040.03\n","Training Loss on iteration  747 209045761461019.34\n","Training Loss on iteration  748 209045761460998.62\n","Training Loss on iteration  749 209045761460977.94\n","Training Loss on iteration  750 209045761460957.25\n","Training Loss on iteration  751 209045761460936.62\n","Training Loss on iteration  752 209045761460916.0\n","Training Loss on iteration  753 209045761460895.34\n","Training Loss on iteration  754 209045761460874.75\n","Training Loss on iteration  755 209045761460854.12\n","Training Loss on iteration  756 209045761460833.56\n","Training Loss on iteration  757 209045761460812.97\n","Training Loss on iteration  758 209045761460792.44\n","Training Loss on iteration  759 209045761460771.88\n","Training Loss on iteration  760 209045761460751.38\n","Training Loss on iteration  761 209045761460730.8\n","Training Loss on iteration  762 209045761460710.34\n","Training Loss on iteration  763 209045761460689.84\n","Training Loss on iteration  764 209045761460669.38\n","Training Loss on iteration  765 209045761460648.94\n","Training Loss on iteration  766 209045761460628.47\n","Training Loss on iteration  767 209045761460608.03\n","Training Loss on iteration  768 209045761460587.62\n","Training Loss on iteration  769 209045761460567.22\n","Training Loss on iteration  770 209045761460546.88\n","Training Loss on iteration  771 209045761460526.44\n","Training Loss on iteration  772 209045761460506.1\n","Training Loss on iteration  773 209045761460485.75\n","Training Loss on iteration  774 209045761460465.44\n","Training Loss on iteration  775 209045761460445.12\n","Training Loss on iteration  776 209045761460424.84\n","Training Loss on iteration  777 209045761460404.56\n","Training Loss on iteration  778 209045761460384.25\n","Training Loss on iteration  779 209045761460364.0\n","Training Loss on iteration  780 209045761460343.72\n","Training Loss on iteration  781 209045761460323.53\n","Training Loss on iteration  782 209045761460303.3\n","Training Loss on iteration  783 209045761460283.12\n","Training Loss on iteration  784 209045761460262.94\n","Training Loss on iteration  785 209045761460242.75\n","Training Loss on iteration  786 209045761460222.56\n","Training Loss on iteration  787 209045761460202.4\n","Training Loss on iteration  788 209045761460182.28\n","Training Loss on iteration  789 209045761460162.16\n","Training Loss on iteration  790 209045761460142.06\n","Training Loss on iteration  791 209045761460121.97\n","Training Loss on iteration  792 209045761460101.9\n","Training Loss on iteration  793 209045761460081.84\n","Training Loss on iteration  794 209045761460061.78\n","Training Loss on iteration  795 209045761460041.72\n","Training Loss on iteration  796 209045761460021.7\n","Training Loss on iteration  797 209045761460001.7\n","Training Loss on iteration  798 209045761459981.72\n","Training Loss on iteration  799 209045761459961.72\n","Training Loss on iteration  800 209045761459941.78\n","Training Loss on iteration  801 209045761459921.8\n","Training Loss on iteration  802 209045761459901.84\n","Training Loss on iteration  803 209045761459881.94\n","Training Loss on iteration  804 209045761459862.0\n","Training Loss on iteration  805 209045761459842.12\n","Training Loss on iteration  806 209045761459822.22\n","Training Loss on iteration  807 209045761459802.38\n","Training Loss on iteration  808 209045761459782.47\n","Training Loss on iteration  809 209045761459762.66\n","Training Loss on iteration  810 209045761459742.84\n","Training Loss on iteration  811 209045761459723.03\n","Training Loss on iteration  812 209045761459703.2\n","Training Loss on iteration  813 209045761459683.4\n","Training Loss on iteration  814 209045761459663.66\n","Training Loss on iteration  815 209045761459643.9\n","Training Loss on iteration  816 209045761459624.16\n","Training Loss on iteration  817 209045761459604.44\n","Training Loss on iteration  818 209045761459584.66\n","Training Loss on iteration  819 209045761459564.97\n","Training Loss on iteration  820 209045761459545.3\n","Training Loss on iteration  821 209045761459525.6\n","Training Loss on iteration  822 209045761459505.97\n","Training Loss on iteration  823 209045761459486.28\n","Training Loss on iteration  824 209045761459466.62\n","Training Loss on iteration  825 209045761459447.0\n","Training Loss on iteration  826 209045761459427.4\n","Training Loss on iteration  827 209045761459407.8\n","Training Loss on iteration  828 209045761459388.22\n","Training Loss on iteration  829 209045761459368.66\n","Training Loss on iteration  830 209045761459349.1\n","Training Loss on iteration  831 209045761459329.56\n","Training Loss on iteration  832 209045761459310.03\n","Training Loss on iteration  833 209045761459290.53\n","Training Loss on iteration  834 209045761459271.03\n","Training Loss on iteration  835 209045761459251.5\n","Training Loss on iteration  836 209045761459232.1\n","Training Loss on iteration  837 209045761459212.6\n","Training Loss on iteration  838 209045761459193.12\n","Training Loss on iteration  839 209045761459173.72\n","Training Loss on iteration  840 209045761459154.25\n","Training Loss on iteration  841 209045761459134.84\n","Training Loss on iteration  842 209045761459115.47\n","Training Loss on iteration  843 209045761459096.1\n","Training Loss on iteration  844 209045761459076.72\n","Training Loss on iteration  845 209045761459057.34\n","Training Loss on iteration  846 209045761459038.0\n","Training Loss on iteration  847 209045761459018.66\n","Training Loss on iteration  848 209045761458999.38\n","Training Loss on iteration  849 209045761458980.06\n","Training Loss on iteration  850 209045761458960.78\n","Training Loss on iteration  851 209045761458941.53\n","Training Loss on iteration  852 209045761458922.25\n","Training Loss on iteration  853 209045761458903.0\n","Training Loss on iteration  854 209045761458883.75\n","Training Loss on iteration  855 209045761458864.53\n","Training Loss on iteration  856 209045761458845.3\n","Training Loss on iteration  857 209045761458826.12\n","Training Loss on iteration  858 209045761458806.9\n","Training Loss on iteration  859 209045761458787.78\n","Training Loss on iteration  860 209045761458768.66\n","Training Loss on iteration  861 209045761458749.5\n","Training Loss on iteration  862 209045761458730.38\n","Training Loss on iteration  863 209045761458711.22\n","Training Loss on iteration  864 209045761458692.12\n","Training Loss on iteration  865 209045761458673.03\n","Training Loss on iteration  866 209045761458653.97\n","Training Loss on iteration  867 209045761458634.88\n","Training Loss on iteration  868 209045761458615.84\n","Training Loss on iteration  869 209045761458596.84\n","Training Loss on iteration  870 209045761458577.78\n","Training Loss on iteration  871 209045761458558.78\n","Training Loss on iteration  872 209045761458539.78\n","Training Loss on iteration  873 209045761458520.78\n","Training Loss on iteration  874 209045761458501.84\n","Training Loss on iteration  875 209045761458482.84\n","Training Loss on iteration  876 209045761458463.9\n","Training Loss on iteration  877 209045761458445.0\n","Training Loss on iteration  878 209045761458426.1\n","Training Loss on iteration  879 209045761458407.16\n","Training Loss on iteration  880 209045761458388.28\n","Training Loss on iteration  881 209045761458369.4\n","Training Loss on iteration  882 209045761458350.53\n","Training Loss on iteration  883 209045761458331.7\n","Training Loss on iteration  884 209045761458312.84\n","Training Loss on iteration  885 209045761458294.03\n","Training Loss on iteration  886 209045761458275.22\n","Training Loss on iteration  887 209045761458256.38\n","Training Loss on iteration  888 209045761458237.6\n","Training Loss on iteration  889 209045761458218.88\n","Training Loss on iteration  890 209045761458200.06\n","Training Loss on iteration  891 209045761458181.3\n","Training Loss on iteration  892 209045761458162.56\n","Training Loss on iteration  893 209045761458143.84\n","Training Loss on iteration  894 209045761458125.16\n","Training Loss on iteration  895 209045761458106.47\n","Training Loss on iteration  896 209045761458087.75\n","Training Loss on iteration  897 209045761458069.12\n","Training Loss on iteration  898 209045761458050.44\n","Training Loss on iteration  899 209045761458031.78\n","Training Loss on iteration  900 209045761458013.16\n","Training Loss on iteration  901 209045761457994.53\n","Training Loss on iteration  902 209045761457975.9\n","Training Loss on iteration  903 209045761457957.34\n","Training Loss on iteration  904 209045761457938.75\n","Training Loss on iteration  905 209045761457920.22\n","Training Loss on iteration  906 209045761457901.6\n","Training Loss on iteration  907 209045761457883.12\n","Training Loss on iteration  908 209045761457864.56\n","Training Loss on iteration  909 209045761457846.03\n","Training Loss on iteration  910 209045761457827.53\n","Training Loss on iteration  911 209045761457809.06\n","Training Loss on iteration  912 209045761457790.6\n","Training Loss on iteration  913 209045761457772.12\n","Training Loss on iteration  914 209045761457753.66\n","Training Loss on iteration  915 209045761457735.22\n","Training Loss on iteration  916 209045761457716.78\n","Training Loss on iteration  917 209045761457698.38\n","Training Loss on iteration  918 209045761457679.97\n","Training Loss on iteration  919 209045761457661.53\n","Training Loss on iteration  920 209045761457643.2\n","Training Loss on iteration  921 209045761457624.84\n","Training Loss on iteration  922 209045761457606.5\n","Training Loss on iteration  923 209045761457588.16\n","Training Loss on iteration  924 209045761457569.8\n","Training Loss on iteration  925 209045761457551.5\n","Training Loss on iteration  926 209045761457533.22\n","Training Loss on iteration  927 209045761457514.94\n","Training Loss on iteration  928 209045761457496.66\n","Training Loss on iteration  929 209045761457478.38\n","Training Loss on iteration  930 209045761457460.12\n","Training Loss on iteration  931 209045761457441.9\n","Training Loss on iteration  932 209045761457423.66\n","Training Loss on iteration  933 209045761457405.5\n","Training Loss on iteration  934 209045761457387.28\n","Training Loss on iteration  935 209045761457369.06\n","Training Loss on iteration  936 209045761457350.9\n","Training Loss on iteration  937 209045761457332.72\n","Training Loss on iteration  938 209045761457314.6\n","Training Loss on iteration  939 209045761457296.44\n","Training Loss on iteration  940 209045761457278.34\n","Training Loss on iteration  941 209045761457260.2\n","Training Loss on iteration  942 209045761457242.12\n","Training Loss on iteration  943 209045761457224.03\n","Training Loss on iteration  944 209045761457205.94\n","Training Loss on iteration  945 209045761457187.88\n","Training Loss on iteration  946 209045761457169.8\n","Training Loss on iteration  947 209045761457151.8\n","Training Loss on iteration  948 209045761457133.78\n","Training Loss on iteration  949 209045761457115.78\n","Training Loss on iteration  950 209045761457097.75\n","Training Loss on iteration  951 209045761457079.78\n","Training Loss on iteration  952 209045761457061.8\n","Training Loss on iteration  953 209045761457043.84\n","Training Loss on iteration  954 209045761457025.9\n","Training Loss on iteration  955 209045761457007.94\n","Training Loss on iteration  956 209045761456990.03\n","Training Loss on iteration  957 209045761456972.1\n","Training Loss on iteration  958 209045761456954.22\n","Training Loss on iteration  959 209045761456936.34\n","Training Loss on iteration  960 209045761456918.47\n","Training Loss on iteration  961 209045761456900.6\n","Training Loss on iteration  962 209045761456882.78\n","Training Loss on iteration  963 209045761456864.9\n","Training Loss on iteration  964 209045761456847.06\n","Training Loss on iteration  965 209045761456829.28\n","Training Loss on iteration  966 209045761456811.47\n","Training Loss on iteration  967 209045761456793.7\n","Training Loss on iteration  968 209045761456775.9\n","Training Loss on iteration  969 209045761456758.12\n","Training Loss on iteration  970 209045761456740.4\n","Training Loss on iteration  971 209045761456722.7\n","Training Loss on iteration  972 209045761456704.9\n","Training Loss on iteration  973 209045761456687.22\n","Training Loss on iteration  974 209045761456669.5\n","Training Loss on iteration  975 209045761456651.84\n","Training Loss on iteration  976 209045761456634.12\n","Training Loss on iteration  977 209045761456616.47\n","Training Loss on iteration  978 209045761456598.84\n","Training Loss on iteration  979 209045761456581.2\n","Training Loss on iteration  980 209045761456563.56\n","Training Loss on iteration  981 209045761456545.94\n","Training Loss on iteration  982 209045761456528.3\n","Training Loss on iteration  983 209045761456510.72\n","Training Loss on iteration  984 209045761456493.16\n","Training Loss on iteration  985 209045761456475.6\n","Training Loss on iteration  986 209045761456458.03\n","Training Loss on iteration  987 209045761456440.47\n","Training Loss on iteration  988 209045761456422.97\n","Training Loss on iteration  989 209045761456405.4\n","Training Loss on iteration  990 209045761456387.9\n","Training Loss on iteration  991 209045761456370.4\n","Training Loss on iteration  992 209045761456352.9\n","Training Loss on iteration  993 209045761456335.47\n","Training Loss on iteration  994 209045761456318.0\n","Training Loss on iteration  995 209045761456300.6\n","Training Loss on iteration  996 209045761456283.16\n","Training Loss on iteration  997 209045761456265.7\n","Training Loss on iteration  998 209045761456248.25\n","Training Loss on iteration  999 209045761456230.9\n","Training Loss on iteration  1000 209045761456213.5\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vanvD93FV0_k","colab_type":"text"},"source":["### Observe values of Weight\n","- Print the updated values"]},{"cell_type":"code","metadata":{"id":"QSqpy4gtWaOD","colab_type":"code","outputId":"abef9025-7d32-4e9e-f321-5c2974c428bd","executionInfo":{"status":"ok","timestamp":1580234275992,"user_tz":-330,"elapsed":1141,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}},"colab":{"base_uri":"https://localhost:8080/","height":101}},"source":["w"],"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(4, 1), dtype=float64, numpy=\n","array([[1731.64040481],\n","       [2062.87231976],\n","       [1532.23682441],\n","       [2287.57121108]])>"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"y9KpRupYUEwy"},"source":["### Observe values of Bias\n","- Print the updated values"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bhEWkGqHWohg","outputId":"ec571fd9-b937-4e18-b4c1-0ff8c6a854f1","executionInfo":{"status":"ok","timestamp":1580234279364,"user_tz":-330,"elapsed":1280,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["b"],"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1,), dtype=float64, numpy=array([5314040.41586172])>"]},"metadata":{"tags":[]},"execution_count":63}]},{"cell_type":"code","metadata":{"id":"Vs6_OoGLiSM7","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}